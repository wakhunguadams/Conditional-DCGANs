{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional DCGAN for Prostate Cancer Biopsy Image Synthesis\n",
    "\n",
    "This notebook implements a Conditional Deep Convolutional GAN (CDCGAN) for synthesizing prostate cancer histopathology images using the PANDA dataset.\n",
    "\n",
    "## Features\n",
    "- Conditional generation by ISUP grade (0-5)\n",
    "- Spectral normalization for training stability\n",
    "- Data augmentation\n",
    "- FID score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision tqdm matplotlib numpy pandas pillow scikit-image scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.nn.utils import spectral_norm\n",
    "from scipy import linalg\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    image_size = 256\n",
    "    nc = 3\n",
    "    nz = 128\n",
    "    ngf = 64\n",
    "    ndf = 64\n",
    "    num_classes = 6\n",
    "    embed_dim = 128\n",
    "    batch_size = 16\n",
    "    num_epochs = 200\n",
    "    lr_g = 0.0002\n",
    "    lr_d = 0.0002\n",
    "    beta1 = 0.5\n",
    "    beta2 = 0.999\n",
    "    label_smoothing = 0.1\n",
    "    checkpoint_dir = './checkpoints'\n",
    "    samples_dir = './samples'\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(config.samples_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Paths and Download Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './panda_data'\n",
    "TRAIN_IMAGES_DIR = os.path.join(DATA_DIR, 'train_images')\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, 'train.csv')\n",
    "PATCHES_DIR = os.path.join(DATA_DIR, 'patches_256')\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PATCHES_DIR, exist_ok=True)\n",
    "\n",
    "print('Download PANDA dataset from:')\n",
    "print('https://www.kaggle.com/competitions/prostate-cancer-grade-assessment/data')\n",
    "print('\\nUsing Kaggle API:')\n",
    "print('!pip install kaggle')\n",
    "print('!kaggle competitions download -c prostate-cancer-grade-assessment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Extraction from WSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image_path, patch_size=256, max_patches=20, tissue_thresh=0.5):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        return []\n",
    "    \n",
    "    w, h = img.size\n",
    "    patches = []\n",
    "    \n",
    "    for i in range((h - patch_size) // patch_size + 1):\n",
    "        for j in range((w - patch_size) // patch_size + 1):\n",
    "            x, y = j * patch_size, i * patch_size\n",
    "            if x + patch_size > w or y + patch_size > h:\n",
    "                continue\n",
    "            \n",
    "            patch = img.crop((x, y, x + patch_size, y + patch_size))\n",
    "            arr = np.array(patch)\n",
    "            tissue_ratio = np.mean(np.mean(arr, axis=2) < 220)\n",
    "            \n",
    "            if tissue_ratio >= tissue_thresh and np.var(arr) > 100:\n",
    "                patches.append((patch, tissue_ratio))\n",
    "    \n",
    "    patches.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [p[0] for p in patches[:max_patches]]\n",
    "\n",
    "\n",
    "def preprocess_dataset(csv_path, images_dir, output_dir, max_per_class=500):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f'Total images: {len(df)}')\n",
    "    print(df['isup_grade'].value_counts().sort_index())\n",
    "    \n",
    "    for g in range(6):\n",
    "        os.makedirs(os.path.join(output_dir, str(g)), exist_ok=True)\n",
    "    \n",
    "    counts = {i: 0 for i in range(6)}\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        grade = row['isup_grade']\n",
    "        if counts[grade] >= max_per_class:\n",
    "            continue\n",
    "        \n",
    "        img_path = os.path.join(images_dir, f\"{row['image_id']}.tiff\")\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        \n",
    "        for idx, patch in enumerate(extract_patches(img_path)):\n",
    "            patch.save(os.path.join(output_dir, str(grade), f\"{row['image_id']}_p{idx}.png\"))\n",
    "        counts[grade] += 1\n",
    "    \n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PANDADataset(Dataset):\n",
    "    def __init__(self, patches_dir, transform=None, balance=True):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        for grade in range(6):\n",
    "            gdir = os.path.join(patches_dir, str(grade))\n",
    "            if os.path.exists(gdir):\n",
    "                for f in os.listdir(gdir):\n",
    "                    if f.endswith(('.png', '.jpg')):\n",
    "                        self.samples.append((os.path.join(gdir, f), grade))\n",
    "        \n",
    "        if balance and self.samples:\n",
    "            counts = {}\n",
    "            for _, g in self.samples:\n",
    "                counts[g] = counts.get(g, 0) + 1\n",
    "            max_c = max(counts.values())\n",
    "            balanced = []\n",
    "            for g in range(6):\n",
    "                gs = [s for s in self.samples if s[1] == g]\n",
    "                if gs:\n",
    "                    balanced.extend(gs * (max_c // len(gs) + 1))\n",
    "            random.shuffle(balanced)\n",
    "            self.samples = balanced[:max_c * 6]\n",
    "        \n",
    "        print(f'Loaded {len(self.samples)} samples')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def get_transforms(size, augment=True):\n",
    "    if augment:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((size, size)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc, num_classes, embed_dim):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, embed_dim)\n",
    "        inp = nz + embed_dim\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(inp, ngf*32, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf*32), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*32, ngf*16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*16), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*16, ngf*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*8), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*4), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "                nn.init.normal_(m.weight, 0, 0.02)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight, 1, 0.02)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        emb = self.label_emb(labels)\n",
    "        x = torch.cat([z, emb], 1).view(z.size(0), -1, 1, 1)\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, nc, num_classes, img_size):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.label_emb = nn.Embedding(num_classes, img_size * img_size)\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(nc+1, ndf, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf*2), nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf*4), nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf*8), nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv2d(ndf*8, ndf*16, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf*16), nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv2d(ndf*16, ndf*32, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf*32), nn.LeakyReLU(0.2, True),\n",
    "            spectral_norm(nn.Conv2d(ndf*32, 1, 4, 1, 0, bias=False)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) and not hasattr(m, 'weight_orig'):\n",
    "                nn.init.normal_(m.weight, 0, 0.02)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight, 1, 0.02)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, imgs, labels):\n",
    "        bs = imgs.size(0)\n",
    "        lmap = self.label_emb(labels).view(bs, 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([imgs, lmap], 1)\n",
    "        return self.main(x).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(config.nz, config.ngf, config.nc, config.num_classes, config.embed_dim).to(device)\n",
    "D = Discriminator(config.ndf, config.nc, config.num_classes, config.image_size).to(device)\n",
    "\n",
    "print(f'Generator params: {sum(p.numel() for p in G.parameters()):,}')\n",
    "print(f'Discriminator params: {sum(p.numel() for p in D.parameters()):,}')\n",
    "\n",
    "# Test\n",
    "z = torch.randn(2, config.nz, device=device)\n",
    "y = torch.randint(0, 6, (2,), device=device)\n",
    "fake = G(z, y)\n",
    "print(f'G output: {fake.shape}')\n",
    "print(f'D output: {D(fake, y).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "opt_G = optim.Adam(G.parameters(), lr=config.lr_g, betas=(config.beta1, config.beta2))\n",
    "opt_D = optim.Adam(D.parameters(), lr=config.lr_d, betas=(config.beta1, config.beta2))\n",
    "sched_G = optim.lr_scheduler.StepLR(opt_G, 50, 0.5)\n",
    "sched_D = optim.lr_scheduler.StepLR(opt_D, 50, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(real_imgs, labels):\n",
    "    bs = real_imgs.size(0)\n",
    "    real_imgs = real_imgs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    real_lbl = torch.full((bs, 1), 1 - config.label_smoothing, device=device)\n",
    "    fake_lbl = torch.zeros(bs, 1, device=device)\n",
    "    \n",
    "    # Train D\n",
    "    D.zero_grad()\n",
    "    out_real = D(real_imgs, labels)\n",
    "    loss_real = criterion(out_real, real_lbl)\n",
    "    \n",
    "    z = torch.randn(bs, config.nz, device=device)\n",
    "    fake = G(z, labels)\n",
    "    out_fake = D(fake.detach(), labels)\n",
    "    loss_fake = criterion(out_fake, fake_lbl)\n",
    "    \n",
    "    loss_D = loss_real + loss_fake\n",
    "    loss_D.backward()\n",
    "    opt_D.step()\n",
    "    \n",
    "    # Train G\n",
    "    G.zero_grad()\n",
    "    z = torch.randn(bs, config.nz, device=device)\n",
    "    fake = G(z, labels)\n",
    "    out = D(fake, labels)\n",
    "    loss_G = criterion(out, real_lbl)\n",
    "    loss_G.backward()\n",
    "    opt_G.step()\n",
    "    \n",
    "    return loss_D.item(), loss_G.item(), out_real.mean().item(), out_fake.mean().item()\n",
    "\n",
    "\n",
    "def generate_samples(n_per_class=4):\n",
    "    G.eval()\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for g in range(6):\n",
    "            z = torch.randn(n_per_class, config.nz, device=device)\n",
    "            y = torch.full((n_per_class,), g, dtype=torch.long, device=device)\n",
    "            samples.append(G(z, y))\n",
    "    G.train()\n",
    "    return torch.cat(samples)\n",
    "\n",
    "\n",
    "def save_grid(samples, epoch):\n",
    "    samples = (samples + 1) / 2\n",
    "    grid = vutils.make_grid(samples.clamp(0, 1), nrow=4, padding=2)\n",
    "    plt.figure(figsize=(12, 18))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Epoch {epoch}')\n",
    "    plt.savefig(f'{config.samples_dir}/epoch_{epoch:04d}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, epochs):\n",
    "    history = {'d_loss': [], 'g_loss': [], 'd_real': [], 'd_fake': []}\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        d_losses, g_losses, d_reals, d_fakes = [], [], [], []\n",
    "        \n",
    "        for imgs, labels in dataloader:\n",
    "            ld, lg, dr, df = train_step(imgs, labels)\n",
    "            d_losses.append(ld)\n",
    "            g_losses.append(lg)\n",
    "            d_reals.append(dr)\n",
    "            d_fakes.append(df)\n",
    "        \n",
    "        history['d_loss'].append(np.mean(d_losses))\n",
    "        history['g_loss'].append(np.mean(g_losses))\n",
    "        history['d_real'].append(np.mean(d_reals))\n",
    "        history['d_fake'].append(np.mean(d_fakes))\n",
    "        \n",
    "        sched_G.step()\n",
    "        sched_D.step()\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f'[{epoch}/{epochs}] D:{history[\"d_loss\"][-1]:.4f} G:{history[\"g_loss\"][-1]:.4f}')\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            save_grid(generate_samples(), epoch)\n",
    "        \n",
    "        if epoch % 25 == 0:\n",
    "            torch.save({'G': G.state_dict(), 'D': D.state_dict(), 'epoch': epoch},\n",
    "                       f'{config.checkpoint_dir}/ckpt_{epoch}.pt')\n",
    "    \n",
    "    torch.save(G.state_dict(), f'{config.checkpoint_dir}/G_final.pt')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.plot(history['d_loss'], label='D')\n",
    "    ax1.plot(history['g_loss'], label='G')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Losses')\n",
    "    \n",
    "    ax2.plot(history['d_real'], label='D(x)')\n",
    "    ax2.plot(history['d_fake'], label='D(G(z))')\n",
    "    ax2.axhline(0.5, c='r', ls='--', alpha=0.5)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend()\n",
    "    ax2.set_title('Discriminator Output')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{config.samples_dir}/history.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_by_grade(n=4):\n",
    "    G.eval()\n",
    "    grades = ['0-Benign', '1-G3+3', '2-G3+4', '3-G4+3', '4-G4+4', '5-High']\n",
    "    fig, axes = plt.subplots(6, n, figsize=(n*3, 18))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for g in range(6):\n",
    "            z = torch.randn(n, config.nz, device=device)\n",
    "            y = torch.full((n,), g, dtype=torch.long, device=device)\n",
    "            imgs = ((G(z, y) + 1) / 2).clamp(0, 1)\n",
    "            for i in range(n):\n",
    "                axes[g, i].imshow(imgs[i].permute(1, 2, 0).cpu())\n",
    "                axes[g, i].axis('off')\n",
    "            axes[g, 0].set_ylabel(grades[g])\n",
    "    \n",
    "    plt.suptitle('Generated by ISUP Grade')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{config.samples_dir}/by_grade.png', dpi=150)\n",
    "    plt.show()\n",
    "    G.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FIDCalculator:\n",
    "    def __init__(self):\n",
    "        self.inception = inception_v3(pretrained=True, transform_input=False)\n",
    "        self.inception.fc = nn.Identity()\n",
    "        self.inception = self.inception.to(device).eval()\n",
    "        self.resize = transforms.Resize((299, 299))\n",
    "        self.norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    \n",
    "    def get_features(self, imgs):\n",
    "        imgs = (imgs + 1) / 2\n",
    "        imgs = self.norm(self.resize(imgs))\n",
    "        with torch.no_grad():\n",
    "            return self.inception(imgs)\n",
    "    \n",
    "    def compute(self, real_loader, generator, n=1000):\n",
    "        generator.eval()\n",
    "        \n",
    "        real_feats = []\n",
    "        cnt = 0\n",
    "        for imgs, _ in real_loader:\n",
    "            if cnt >= n: break\n",
    "            real_feats.append(self.get_features(imgs.to(device)))\n",
    "            cnt += imgs.size(0)\n",
    "        real_feats = torch.cat(real_feats)[:n].cpu().numpy()\n",
    "        \n",
    "        fake_feats = []\n",
    "        with torch.no_grad():\n",
    "            while len(fake_feats) * 32 < n:\n",
    "                z = torch.randn(32, config.nz, device=device)\n",
    "                y = torch.randint(0, 6, (32,), device=device)\n",
    "                fake_feats.append(self.get_features(generator(z, y)))\n",
    "        fake_feats = torch.cat(fake_feats)[:n].cpu().numpy()\n",
    "        \n",
    "        mu1, s1 = np.mean(real_feats, 0), np.cov(real_feats, rowvar=False)\n",
    "        mu2, s2 = np.mean(fake_feats, 0), np.cov(fake_feats, rowvar=False)\n",
    "        \n",
    "        diff = mu1 - mu2\n",
    "        covmean = linalg.sqrtm(s1.dot(s2))\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        \n",
    "        generator.train()\n",
    "        return diff.dot(diff) + np.trace(s1) + np.trace(s2) - 2 * np.trace(covmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(output_dir, n_per_class=100):\n",
    "    G.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for g in range(6):\n",
    "        gdir = os.path.join(output_dir, f'grade_{g}')\n",
    "        os.makedirs(gdir, exist_ok=True)\n",
    "        \n",
    "        print(f'Grade {g}...')\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(n_per_class)):\n",
    "                z = torch.randn(1, config.nz, device=device)\n",
    "                y = torch.tensor([g], device=device)\n",
    "                img = ((G(z, y).squeeze() + 1) / 2).clamp(0, 1)\n",
    "                img = (img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "                Image.fromarray(img).save(f'{gdir}/syn_{g}_{i:04d}.png')\n",
    "    \n",
    "    G.train()\n",
    "    print(f'Generated {n_per_class * 6} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoDataset(Dataset):\n",
    "    def __init__(self, n=600, size=256):\n",
    "        self.n = n\n",
    "        self.size = size\n",
    "        self.colors = [[.9,.8,.9],[.85,.7,.85],[.8,.6,.8],[.75,.5,.75],[.7,.4,.7],[.65,.3,.65]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        g = i % 6\n",
    "        base = np.array(self.colors[g])\n",
    "        img = np.clip(base + np.random.randn(self.size, self.size, 3) * 0.1, 0, 1)\n",
    "        \n",
    "        for _ in range(20 + g * 10):\n",
    "            cx, cy = np.random.randint(0, self.size, 2)\n",
    "            r = np.random.randint(3, 10)\n",
    "            yy, xx = np.ogrid[-cy:self.size-cy, -cx:self.size-cx]\n",
    "            mask = xx**2 + yy**2 <= r**2\n",
    "            img[mask] = np.clip(img[mask] - 0.2, 0, 1)\n",
    "        \n",
    "        return torch.from_numpy(img).float().permute(2, 0, 1) * 2 - 1, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run demo\n",
    "RUN_DEMO = True\n",
    "\n",
    "if RUN_DEMO:\n",
    "    print('Running demo with synthetic data...')\n",
    "    demo_loader = DataLoader(DemoDataset(600, config.image_size), \n",
    "                             batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    for epoch in range(1, 6):\n",
    "        losses_d, losses_g = [], []\n",
    "        for imgs, labels in demo_loader:\n",
    "            ld, lg, _, _ = train_step(imgs, labels)\n",
    "            losses_d.append(ld)\n",
    "            losses_g.append(lg)\n",
    "        print(f'Epoch {epoch}: D={np.mean(losses_d):.4f} G={np.mean(losses_g):.4f}')\n",
    "    \n",
    "    print('\\nGenerating samples...')\n",
    "    visualize_by_grade(4)\n",
    "    print('Demo complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Real Data\n",
    "\n",
    "Uncomment and run after downloading the PANDA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess (run once)\n",
    "# if os.path.exists(TRAIN_CSV):\n",
    "#     preprocess_dataset(TRAIN_CSV, TRAIN_IMAGES_DIR, PATCHES_DIR)\n",
    "\n",
    "# Create dataloader\n",
    "# transform = get_transforms(config.image_size)\n",
    "# dataset = PANDADataset(PATCHES_DIR, transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "# Train\n",
    "# history = train(dataloader, config.num_epochs)\n",
    "# plot_history(history)\n",
    "\n",
    "# Evaluate\n",
    "# fid_calc = FIDCalculator()\n",
    "# fid = fid_calc.compute(dataloader, G)\n",
    "# print(f'FID: {fid:.2f}')\n",
    "\n",
    "# Generate synthetic data\n",
    "# generate_dataset('./synthetic_data', n_per_class=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This CDCGAN implementation includes:\n",
    "1. **Data Pipeline**: WSI patch extraction, augmentation, class balancing\n",
    "2. **Generator**: Label embedding + transposed convolutions (256x256)\n",
    "3. **Discriminator**: Spectral normalization for stability\n",
    "4. **Training**: BCE loss with label smoothing\n",
    "5. **Evaluation**: FID score\n",
    "\n",
    "### Next Steps\n",
    "1. Download PANDA dataset from Kaggle\n",
    "2. Run preprocessing\n",
    "3. Train the model\n",
    "4. Generate synthetic data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
